---
layout:     post
title:      近邻分类
subtitle:   机器学习算法
date:       2017-11-07
author:     WSS
header-img: img/AI4.jpg
catalog: true
tags:
    - Machine Learning
	- KNN
---

## 算法简介 ##

k-近邻算法是用于分类。分类问题是监督学习算法的一个研究方向。既然是属于监督学习的，就需要1个训练数据集作为判断的基础。

kNN的学习方法很朴素，简单说就是站队，对于已有的训练集，当传入未知数据待分类时，分别计算未知数据和训练集所有数据点的距离，将距离从小到大取前k个。根据这k个点的分类情况判断未知数据的分类。通常哪个类别在前k个数据点中数量最多，我们就认为未知数据是哪个类别的。

![](http://oyug2kd6x.bkt.clouddn.com//MachineLearning/KNNMachineLearingKNN.jpg)

如图，在拥有三个分类的若干样本点的训练集中，对未知元素Xu，我们取k=5，选取距离其最近的5个训练集的点，其中4个是红色圆圈，1个是绿色方块，0个蓝色三角，那么我们认为，其最可能所属的分类是红色圆圈。

## 适用 ##

一般来说，近邻分类器比较适合：特征和目标之间的关系众多、复杂。用其他方式难以理解，但是具有相似类的项目又是非常近似，也就是说，一个概念很难理解，但是当你看到它时你知道他是什么，那么近邻分类就是合适的方法。但是如果组与组之间没有明确的界限，那么该算法就不太合适。

目前，近邻分类已经成功应用在：

·计算机视觉应用，包括在静止图像和视频中的光学字符识别和面部识别。
·预测一个人是否喜欢推荐给他们的电影。
·识别基因数据的模式，用于发现特定的蛋白质或疾病。

优点 | 缺点
---- | ---
简单有效 | 不产生模型，在发现特征之间关系上的能力有限
对数据的分布没有要求 |  分类阶段很慢，需要大量内存
训练阶段很快 |  **名义变量（特征）**和缺失数据需要额外处理

名义变量：

对于名义特征，如 性别，类别或其他属性需要将其转换为数值型格式。一种典型的解决方法就是利用哑变量编码（dummy coding）,其中1表示一个类别,0表示其他类别，例如，可以构建性别变量的哑变量编码：

## 一个例子 ##

