---
layout:     post
title:      RBFN(径向基函数神经网络)
subtitle:   Radial Basis Function Net
date:       2018-03-19
author:     WSS
header-img: img/post-bg-pandas.jpg
catalog: true
tags:
    - RBFN
    - Python
	- TensorFlow
---


## RBF介绍 ##

径向基函数（Radical Basis Function，RBF）方法是Powell在1985年提出的。所谓径向基函数，其实就是某种沿径向对称的标量函数。通常定义为空间中任一点x到某一中心c之间欧氏距离的单调函数，可记作k(||x-c||)，其作用往往是局部的，即当x远离c时函数取值很小。所以其输出结果是一组很平滑的小数，在特定的输入值(中心点)处具有最大的函数值, 输入离这个特定的值越远，输出就成指数下降。  例如高斯径向基函数：
![](http://oyug2kd6x.bkt.clouddn.com//RBFN/RBF.png)

径向基函数的诞生主要是为了解决多变量插值的问题。可以看下面的图。具体的话是先在每个样本上面放一个基函数，图中每个蓝色的点是一个样本，然后中间那个图中绿色虚线对应的，就表示的是每个训练样本对应一个高斯函数（高斯函数中心就是样本点）。然后假设真实的拟合这些训练数据的曲线是蓝色的那根（最右边的图），如果我们有一个新的数据x1，我们想知道它对应的f(x1)是多少，也就是a点的纵坐标是多少。那么由图可以看到，a点的纵坐标等于b点的纵坐标加上c点的纵坐标。而b的纵坐标是第一个样本点的高斯函数的值乘以一个大点权值得到的，c的纵坐标是第二个样本点的高斯函数的值乘以另一个小点的权值得到。而其他样本点的权值全是0，因为我们要插值的点x1在第一和第二个样本点之间，远离其他的样本点，那么插值影响最大的就是离得近的点，离的远的就没什么贡献了。所以x1点的函数值由附近的b和c两个点就可以确定了。拓展到任意的新的x，这些红色的高斯函数乘以一个权值后再在对应的x地方加起来，就可以完美的拟合真实的函数曲线了。

![](http://oyug2kd6x.bkt.clouddn.com//RBFN/rbf-1.png)

## RBFN ##

有了上述RBF的概念，以及插值问题的介绍，下面直接引入RBF Network:

![](http://oyug2kd6x.bkt.clouddn.com//RBFN/RBFN.png)


####  RBFN与ANN对比 ####

首先网络结构很类似，都是输入层, 隐藏层，输出层构成，此外最后一层的输出层基本类似类似，均是对隐层的输出线性组合(权重控制)然后得到求和的结果。

不过一般情况RBF Network只有三层，其中从输入层到隐层之间并没有权重连接，而是直接将用隐层的RBF计算与不同的中心(隐层神经元)的距离或者相似度，距离越远，相似度越低，神经元的激活程度就越小，作用也就越不明显，RBF Network的隐层激活函一般为径向基函数，这些RBF函数的核宽都是一样的，它们中心一般是每个训练样本点，或者是训练样本点的聚类中心。对比一般ANN的激活函数则是一些如sigmod, tanh等非线性函数。

RBF Network由于只有隐层到输出层的权重连接，且层数较少，因而训练速度会大大加快。从另一个角度来看，一个神经元，只负责对某一块进行响应。速度当然快得多。大脑里的神经元就是这么工作的。闻到花香的时候，不会刺激到感受辣味的神经元。

![](http://oyug2kd6x.bkt.clouddn.com//RBFN/rbf-2&ann.png)

####  RBFN与SVM对比  ####

RBFN隐层的RBF计算与不同的中心(隐层神经元)的距离，这个过程也可以以Kernel SVM的角度理解: 把原始低维的数据进行转换到高维空间中(高斯核对应无穷维)的特征转换。

若只看模型，RBF Network 与 SVM with RBF kernel 无异。他们的区别主要在训练方式。

## RBFN设计 ##

RBF的隐层神经元也就是center的选择是个很关键的问题，只有中心确定了之后，RBF函数才能够确定。下面基于不同的中心介绍两种类型的RBF。

#### Full RBF Network ####

Full RBF Network，顾名思义便是所有的数据节点都作为中心。



>
>
