---
layout:     post
title:      	RBFN
subtitle:   Radial Basis Function Net
date:       2018-03-19
author:     WSS
header-img: img/post-bg-os-metro.jpg
catalog: true
tags:
    - TensorFlow
    - RBFN
---

## RBF介绍 ##

径向基函数（Radical Basis Function，RBF）其实就是某种沿径向对称的标量函数。通常定义为空间中任一点x到某一中心c之间欧氏距离的单调函数，可记作k(||x-c||)，其作用往往是局部的，即当x远离c时函数取值很小。所以其输出结果是一组很平滑的小数，在特定的输入值(中心点)处具有最大的函数值, 输入离这个特定的值越远，输出就成指数下降。  例如高斯径向基函数：
![](http://oyug2kd6x.bkt.clouddn.com//RBFN/RBF.png)

径向基函数的诞生主要是为了解决多变量插值的问题。可以看下面的图。具体的话是先在每个样本上面放一个基函数，图中每个蓝色的点是一个样本，然后中间那个图中绿色虚线对应的，就表示的是每个训练样本对应一个高斯函数（高斯函数中心就是样本点）。然后假设真实的拟合这些训练数据的曲线是蓝色的那根（最右边的图），如果我们有一个新的数据x1，我们想知道它对应的f(x1)是多少，也就是a点的纵坐标是多少。那么由图可以看到，a点的纵坐标等于b点的纵坐标加上c点的纵坐标。而b的纵坐标是第一个样本点的高斯函数的值乘以一个大点权值得到的，c的纵坐标是第二个样本点的高斯函数的值乘以另一个小点的权值得到。而其他样本点的权值全是0，因为我们要插值的点x1在第一和第二个样本点之间，远离其他的样本点，那么插值影响最大的就是离得近的点，离的远的就没什么贡献了。所以x1点的函数值由附近的b和c两个点就可以确定了。拓展到任意的新的x，这些红色的高斯函数乘以一个权值后再在对应的x地方加起来，就可以完美的拟合真实的函数曲线了。

![](http://oyug2kd6x.bkt.clouddn.com//RBFN/rbf-1.png)

## RBFN ##

有了上述RBF的概念，以及插值问题的介绍，下面直接引入RBF Network:

![](http://oyug2kd6x.bkt.clouddn.com//RBFN/RBFN.png)


####  RBFN与ANN对比 ####

首先网络结构很类似，都是输入层, 隐藏层，输出层构成，此外最后一层的输出层基本类似类似，均是对隐层的输出线性组合(权重控制)然后得到求和的结果。

不过一般情况RBF Network只有三层，其中从输入层到隐层之间并没有权重连接，而是直接将用隐层的RBF计算与不同的中心(隐层神经元)的距离或者相似度，距离越远，相似度越低，神经元的激活程度就越小，作用也就越不明显，RBF Network的隐层激活函一般为径向基函数，这些RBF函数的核宽都是一样的，它们中心一般是每个训练样本点，或者是训练样本点的聚类中心。对比一般ANN的激活函数则是一些如sigmod, tanh等非线性函数。

RBF Network由于只有隐层到输出层的权重连接，且层数较少，因而训练速度会大大加快。从另一个角度来看，一个神经元，只负责对某一块进行响应。速度当然快得多。大脑里的神经元就是这么工作的。闻到花香的时候，不会刺激到感受辣味的神经元。

![](http://oyug2kd6x.bkt.clouddn.com//RBFN/rbf-2&ann.png)

####  RBFN与SVM对比  ####

RBFN隐层的RBF计算与不同的中心(隐层神经元)的距离，这个过程也可以以Kernel SVM的角度理解: 把原始低维的数据进行转换到高维空间中(高斯核对应无穷维)的特征转换。

若只看模型，RBF Network 与 SVM with RBF kernel 无异。他们的区别主要在训练方式。

## RBFN设计 ##

RBFN的计算主要为：

![](http://oyug2kd6x.bkt.clouddn.com//RBFN/rbfngongshi.png)

又两个重要的参数，um与bm。

RBF的隐层神经元也就是center(um)的选择是个很关键的问题，只有中心确定了之后，RBF函数才能够确定。下面基于不同的中心介绍两种类型的RBF。

#### Full RBF Network ####

Full RBF Network，顾名思义便是所有的数据节点都作为中心。

预测新的数据点，需要计算该点与所有训练数据点的距离，也就是相似度，然后结合权重β进行线性组合，此过程就是将所有的训练数据点对预测点的影响聚集到一起，距离越近，影响越大，这样得到最终的结果。比如用均匀影响做分类的话，即βm=1⋅ym。

这种Full RBF Network是一种偷懒的方式，因为直接将所有的点作为了center，因此如果样本量很大的话，那么计算量就太大了，在实际中，很少使用。

#### K Nearest Neighbor(KNN) ####

将上面的方法稍微改变一下，就可以得到另外一种很常见的机器学习方法-KNN(K近邻)。在Full RBF中，我们计算所有的训练数据与新数据的距离，距离最小的与新数据的相似度最高，而且高斯函数衰减很快，距离新数据远的点对它的影响很小，因此我们可以忽略那些，只需要找到几个最靠近新数据的点，然后只计算它们的贡献即可，假设我们找最近的K个点的话， 那就是K近邻算法。这种类型的算法在训练的时候，不用花力气，但是再做测试或者预测的时候，需要对比全部的数据，然后找到几个最近的，计算这几个的贡献得到最后结果，这个过程跟上面的Full RBF 一样，计算量很大。 因此这种方式经常在样本数据较少的时候使用。

#### K-Means ####

上述的几种形式基本都属于偷懒的方式，因为没有花费力气去找一些好的中心，都是使用所有的数据作为RBF的center, 一旦样本量变大，一则计算量变大，二则更容易过拟合，降低泛化性能，因此很有必要花点力气去找一些更加靠谱数量更少的center。

中心点其实就是那些Support Vectors，而非所有的数据。如何去选择中心点，实际就是要将训练点分为几簇，给每一簇要找一个代表，并且簇与簇之间不能有交集，类似与离散数学中的划分的概念，假设所有的数据划分为M个簇:S1,S2,⋯,SM,其中μm为Sm的中心(prototype)，那么我们就希望：

x1,x2∈Sm⇔μm≈x1≈x2

也就是在同一簇中的点，都应该是最相似的。这样我们的问题成为找到这样的簇，以及簇的中心，一个很典型的聚类(Cluster)问题，同时也是无监督的。

除了离散项之外，还需要对S,μ两个变量同时优化，因此原式很难直接求最优解。下面使用一种alternating optimization(交替优化)的方法求解，原理就是先固定某一个变量，然后更新优化另一个，然后反过来，继续交替更新，直到收敛。

首先固定中心点，即μ1,⋯,μM确定，更新S1,⋯,SM的元素，我们需要对每个xn找到它最应该划分到哪个S内，很简单，我们需要找到距离xn最近的那个中心μm即可，这样xn划到Sm内。

然后固定簇，即S1,S2,...,SM确定，更新簇的中心:μ1,μ2...μM。一旦簇确定了，那么xn∈Sm这一项已经确定，要么1要么0，单独考虑其中一项μm。

Sm 的中心等于簇内所有点的平均值。

交替更新完一轮S,μ之后，重复这个过程直到收敛，整个聚类的算法就是著名的K-Means，下面给出基本流程:

![](http://oyug2kd6x.bkt.clouddn.com//RBFN/k-means3.png)

>K的选择，一般需要一些先验知识，影响很大

>μ 初始化很关键，对整个收敛速度与质量影响很大，需要多测试

>收敛性，K-Means可以一定收敛，因为交替更新的两个过程都是在使得Ein下降。不过由于Ein 并不是凸函数，因此我们得到可能不是全局最优解。

>K-Means属于无监督的特征转换过程，类似与AutoEncoder。

>
>
